{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving hard real-world problems with machine learning demands advanced algorithmic designs. These designs should be based on a solid understanding of the principles that determine the behavior of learning algorithms. Statistical learning theory provides mathematical tools to develop such an understanding. It seeks answers to fundamental questions about the nature of learning from data such as\n",
    "\n",
    " - Under which conditions can a learning algorithm generalize from the training data to new data?\n",
    " - How can we measure the complexity of a learning task?\n",
    " - How can we control the complexity of a learning algorithm?\n",
    " - How can we estimate the generalization performance of a learning algorithm?\n",
    "\n",
    "Based on what we covered in the probability theory chapter, let us start from investigating the last question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization bounds\n",
    "\n",
    "Remember the basic definitions below.\n",
    "\n",
    "**Definition 5.1 (Generalization error)** Given a hypothesis $h \\in \\mathcal{H}$, a loss function $\\ell: Y \\times Y \\rightarrow [0,1]$ a data distribution $x,y \\sim D$, the generalization error of $h$ is defined as\n",
    "\n",
    "$R(h) = P_{x,y \\sim D}[\\ell(h(x),y)]$.\n",
    "\n",
    "**Definition 5.2 (Empirical error)** Given a hypothesis $h \\in \\mathcal{H}$, a loss function $\\ell: Y \\times Y \\rightarrow [0,1]$, and a data set $S = \\{(x_1,y_1),...,(x_m,y_m)\\}$, the empirical error of $h$ is defined as\n",
    "\n",
    "$\\widehat{R}_S(h) = \\frac{1}{m} \\sum_{i=1}^m \\ell(h(x_i),y_i)$.\n",
    "\n",
    "Using the definitions above and the concentration inequalities we covered in the probability theory chapter, we can derive the following theorem.\n",
    "\n",
    "**Theorem 5.1 (Generalization bound)** Let $\\mathcal{H}$ be a finite hypothesis set. Then for any $\\delta \\in (0,1)$, with probability at least $1-\\delta$ over the choice of $S \\sim D^m$, for all $h \\in \\mathcal{H}$\n",
    "\n",
    "$R(h) \\leq \\widehat{R}_S(h) + \\sqrt{\\frac{\\log |\\mathcal{H}| + \\log \\frac{1}{\\delta}}{2m}}$.\n",
    "\n",
    "**Proof.** Let $\\mathcal{H} = \\{h_1,...,h_{|\\mathcal{H}|}\\}$.  Then we have\n",
    "\n",
    "\\begin{align*}\n",
    "P_{S \\sim D^m}\\left[\\exists h_i \\in \\mathcal{H}: |\\widehat{R}_S(h_i) - R(h_i)| > \\epsilon \\right ]\n",
    "&=P_{S \\sim D^m}\\left [\\cup_{i=1}^{|\\mathcal{H}|} \\Big \\{ |\\widehat{R}_S(h_i) - R(h_i)| > \\epsilon \\Big \\} \\right ] \\\\\n",
    "&\\leq \\sum_{i=1}^{|\\mathcal{H}|} P_{S \\sim D^m}[ |\\widehat{R}_S(h_i) - R(h_i)| > \\epsilon] \\\\\n",
    "&\\leq |\\mathcal{H}| \\exp(-2m\\epsilon^2).\n",
    "\\end{align*}\n",
    "where the first inequality follows from the union bound and the second from Hoeffding's inequality. Setting the right hand side to $\\delta$ and solving for $\\epsilon$ yields the final result. $\\square$\n",
    "\n",
    "This inequality is called a generalization bound, because it bounds the generalization error of any hypothesis $h \\in \\mathcal{H}$ in terms of its empirical error. We can derive a number of interesting consequences from this bound:\n",
    "\n",
    " - The bound is uniform in the sense that it holds simultaneously for all hypotheses in $\\mathcal{H}$.\n",
    " - The bound is independent of the data distribution $D$ and the loss function $\\ell$.\n",
    " - The bound increases logarithmically with the size of the hypothesis set $|\\mathcal{H}|$, that is, a richer hypothesis set is more likely to overfit.\n",
    " - The bound decreases with the size of the training set $m$, that is, more data is less likely to overfit.\n",
    " - For a fixed training set size $m$ and two different hypothesis sets $\\mathcal{H}_1$ and $\\mathcal{H}_2$, the bound prefers the smaller hypothesis set. This is known as the **Occam's razor principle**. The principle is introduced by the 14th century theologian William of Ockham. It states that among competing hypotheses, the one with the fewest assumptions should be selected.\n",
    " - The bound gives a **Probably Approximately Correct (PAC)** performance guarantee. The event that any hypothesis in $\\mathcal{H}$ is **approximately correct** in the sense that its generalization error is at most $\\epsilon$ with probability at least $1-\\delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAC Learnability\n",
    "\n",
    "**Definition 5.3 (Realizability).** A hypothesis space $\\mathcal{H}$ is realizable with respect to a loss $\\ell$ and data distribution $D$ if $\\exists h^* \\in \\mathcal{H}$ such that $R(h) = 0$. \n",
    "\n",
    "It trivially follows from this definition that $\\min_{h \\in \\mathcal{H}} \\widehat{R}_S(h)=0$  with probability 1 for a sample set $S$ collected i.i.d. from $D$. Hence, under the realizability assumption, all ERM solutions $h_S \\in \\arg \\min_{h \\in \\mathcal{H}} \\widehat{R}_S(h)$ give zero error.\n",
    "\n",
    "---\n",
    "\n",
    "**Definition 5.4 (Representativeness)** A training set $S$ is called $\\epsilon-$ representative if \n",
    "\n",
    "$\\forall h \\in \\mathcal{H}, |R(h) - \\widehat{R}_S(h)| \\leq \\epsilon$.\n",
    "\n",
    "---\n",
    "\n",
    "**Lemma 5.1** Assume that a training set $S$ is $\\frac{\\epsilon}{2}$-representative, then any ERM solution $h_S \\in \\arg \\min_{h \\in \\mathcal{H}} \\widehat{R}_S(h)$ satisfies\n",
    "\n",
    "$\\widehat{R}_S(h) \\leq \\min_{h \\in \\mathcal{H}} R(h) + \\epsilon$\n",
    "\n",
    "***Proof.*** For every $h \\in \\mathcal{H}$,\n",
    "$L_{D}(h_S) \\le L_S(h_S) + \\frac{\\epsilon}{2} \\le L_S(h) + \\frac{\\epsilon}{2} \\le L_{D}(h) + \\frac{\\epsilon}{2} + \\frac{\\epsilon}{2} = L_{D}(h) + \\epsilon.$\n",
    "\n",
    "---\n",
    "\n",
    "**Definition 5.5 (Uniform Convergence).** A hypothesis class $\\mathcal{H}$ has the **uniform convergence property** if there exists a function $m_{\\mathcal{H}}^{\\text{UC}} : (0, 1)^2 \\to \\mathbb{N}$ such that for every $\\epsilon, \\delta \\in (0, 1)$ and every distribution $D$, any sampled data set $S=\\{(x_i,y_i) \\overset{i.i.d.}{\\sim} D : i = 1, \\ldots, m\\}$ with $m \\ge m_{\\mathcal{H}}(\\epsilon, \\delta)$  is $\\epsilon$-representative with probability at least $1-\\delta$.\n",
    "\n",
    "---\n",
    "\n",
    "**Definition 5.6 (Agnostic PAC Learnability)** A hypothesis class $\\mathcal{H}$ is **agnostic PAC learnable** if there exist a function $m_{\\mathcal{H}}: (0, 1)^2 \\to \\mathbb{N}$ and a **learning algorithm** $A$ such that for every $\\epsilon, \\delta \\in (0, 1)$ and every distribution $D$, running the learning algorithm on data set $S=\\{(x_i,y_i) \\overset{i.i.d.}{\\sim} D : i = 1, \\ldots, m\\}$ with $m \\ge m_{\\mathcal{H}}(\\epsilon, \\delta)$ satisfies \n",
    "\n",
    "$P(\\{S: R(A(S)) \\le \\min_{h' \\in \\mathcal{H}} R(h') + \\epsilon\\}) \\geq 1-\\delta.$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Corollary 5.1** If $|\\mathcal{H}| < \\infty$ (finite hypothesis class), then it has the uniform convergence property with sample complexity\n",
    "\n",
    "$m_{\\mathcal{H}}^{\\text{UC}}(\\epsilon, \\delta) \\le \\left\\lceil \\frac{\\log(2|\\mathcal{H}|/\\delta)}{2\\epsilon^2} \\right\\rceil.$\n",
    "\n",
    "and is agnostically PAC learnable using the ERM algorithm with sample complexity\n",
    "\n",
    "$m_{\\mathcal{H}}(\\epsilon, \\delta) \\le m_{\\mathcal{H}}^{\\text{UC}}(\\epsilon/2, \\delta) \\le \\left\\lceil \\frac{2\\log(2|\\mathcal{H}|/\\delta)}{\\epsilon^2} \\right\\rceil.$\n",
    " \n",
    "---\n",
    "\n",
    "**Definition 5.7 (Bayes Error)** Given a data distribution $x,y \\sim D$, the Bayes error is defined as:\n",
    "\n",
    "$R^* = \\min_{h' \\in \\mathcal{H}} R(h')$.\n",
    "\n",
    "A hypothesis $h^* \\in \\mathcal{H}$ that achieves the Bayes error is called a **Bayes (optimal) hypothesis**.\n",
    "\n",
    "For a binary classification problem with $Y = \\{0,1\\}$, the Bayes hypothesis would be\n",
    "\n",
    "$h^*(x) = \\arg \\max_{y \\in \\{0,1\\}} P[y|x]$.\n",
    "\n",
    "Then the related Bayes error is given by\n",
    "\n",
    "$R^* = 1 - \\max_{y \\in \\{0,1\\}} P[y|x] = \\min_{y \\in \\{0,1\\}} P[y|x]$. This quantity is also denoted as **noise**. \n",
    "\n",
    "---\n",
    "\n",
    "**Definition 5.8 (PAC learnability)** A hypothesis class is **PAC learnable** with respect to a data distribution $D$ if it admits zero Bayes error, i.e., $R^* = 0$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Complexity Dilemma (Trade-off)\n",
    "\n",
    "**Theorem 5.2 (No free lunch)** Let $A$ be any learning algorithm for the task of binary classification with respect to the 0/1 loss over a domain $X$ and $m < |X|/2$ be a positive integer representing a training set size. Then, there exists a distribution $D$ over $X \\times \\{0, 1\\}$ such that:\n",
    " * There exists a function $f : X \\to \\{0, 1\\}$ with $R(f) = 0$.\n",
    " * With probability of at least $1/7$ over the choice of $S \\sim D$ we have that $R(A(S)) \\ge 1/8$.\n",
    " \n",
    "---\n",
    "\n",
    "**Corollary 5.2** Let $X$ be an infinite domain ($|X|=\\infty$, e.g. a real-valued feature space) and $\\mathcal{H}$ be the set of all functions from $X$ to $\\{0,1\\}$. Then $\\mathcal{H}$ is not PAC learnable.\n",
    "\n",
    "***Proof (Shalev-Shwartz book).*** Assume, by way of contradiction, that the class is learnable. Choose some $\\epsilon < 1/8$ and $\\delta < 1/7$. By the definition of PAC learnability, there must be some learning algorithm $A$ and an integer $m = m(\\epsilon, \\delta)$, such that for any data-generating distribution over $\\mathcal{X} \\times \\{0, 1\\}$, if for some function $f : X \\to \\{0, 1\\}$, $R(f)=0$, then with probability greater than $1 - \\delta$ when $A$ is applied to samples $S$ of size $m$, generated i.i.d. by $D$, $R(A(S)) \\le \\epsilon$. However, applying the No-Free-Lunch theorem, since $|X| > 2m$, for every learning algorithm (and in particular for the algorithm $A$), there exists a distribution $D$ such that with probability greater than $1/7 > \\delta$, $R(A(S)) > 1/8 > \\epsilon$, which leads to the desired contradiction $\\square$\n",
    "\n",
    "---\n",
    "The take-home message of the above theorem and its corollary is that there is no universal learner that works well for all data distributions. There always exists a task for which the learner fails. This is a very important result. It tells us that we need to make assumptions about the data distribution in order to design a good learner. These assumptions will constitute the **inductive bias** of the learner. The no free lunch theorem tells us only that we need to induce a degree of bias to the learner. However, it does not tell anything about its consequences. Inducing too much bias limits the ability of the learner to explain the training observations. Inducing too little bias leads to overfitting. The goal is to find the right balance between the two. This dilemma is known as the **bias-complexity dilemma**. \n",
    "\n",
    "Let us describe the bias-complexity dilemma in more formal terms. Given an ERM solution $h_S \\in \\arg \\min_{h \\in \\mathcal{H}} \\widehat{R}_S(h)$ for a hypothesis space $\\mathcal{H}$, we can decompose its generalization error as below\n",
    "\n",
    "$\\underbrace{R(h_S)}_{\\text{Generalization error}} = \\underbrace{\\min_{h \\in \\mathcal{H}} R(h)}_{\\text{Approximation error}} + \\underbrace{\\epsilon_{est}}_{\\text{Estimation error}}$\n",
    "\n",
    "where the first term, called the **approximation error** is the Bayes error that stems from the randomness in the data distribution and the chosen hypothesis space. The second term is called the *estimation error** and denoted by $\\epsilon_{est}$. This term is the remainder of the generalization error after the approximation error.  Since for any $\\mathcal{H'} \\supset \\mathcal{H}$, it holds that $\\min_{h' \\in \\mathcal{H}'} R(h') \\leq \\min_{h \\in \\mathcal{H}} R(h)$, we can influence the approximation error via our choices on the hypothesis space. Other things such as the learning algorithm and data distribution being equal, the generalization error will give us a fixed budget. Hence, approximation and estimation error will need to be traded off against each other. Increasing the hypothesis space, thereby making our solution more complex will reduce the approximation error, but will increase the estimation error. Since the realizability assumption implies a smaller training error, we will observe overfitting in this scenario. The opposite will be true when we restrict the hypothesis space.\n",
    "\n",
    "---\n",
    "**Bias-variance decomposition.** The bias-complexity dilemma can also be observed from the bias and variance of the estimated values of a regression output. Consider a regression problem where observations $(x,y) \\sim D$. Our quantity of interest is $E[y|x]$. We are up to estimating this quantity from a training set $S = \\{(x_1,y_1),...,(x_m,y_m)\\}$. For a given test-time sample $x$. Our **estimator** for $E[y|x]$ is a hypothesis $h_S \\in \\mathcal{H}$ that minimizes the mean squared error loss. The expected squared error of the prediction made by this estimator over the noisy label $y$ and training sample $S$ is given by\n",
    "\n",
    "\\begin{align*}\n",
    "     E_{S,  y|x} \\Big [ \\Big(y - h_{S}(x) \\Big)^2 \\Big ]&=E_{S, y|x} \\Big [ y^2 - 2 y h_{S}(x) + h_{S}(x)^2  \\Big ]\\\\\n",
    "    &=E_{y|x}  [ y^2] + E_{S}  [  h_{S}(x)^2  ] - 2 E_{y|x}  [ y ] E_{S}  [ h_{S}(x) ] \\\\\n",
    "    &=E_{y|x}  [ y]^2 + Var_{y|x}[y] + E_{S}  [  h_{S}(x)  ]^2 + Var_{S}  [  h_{S}(x)  ]- 2 E_{y|x}  [ y ]E_{S}  [ h_{S}(x) ]\\\\\n",
    "    &= \\Big ( \\underbrace{E_{y|x}  [ y ] -E_{S}  [ h_{S}(x) ]}_{\\text{Estimator~Bias}} \\Big )^2 + \\underbrace{Var_{S}  [  h_{S}(x)  ]}_{\\text{Estimator~Variance}}+\\underbrace{Var_{y|x}[y]}_{\\text{Label~noise~variance}}\n",
    "\\end{align*}\n",
    "\n",
    "Note that $E_{S|x}[h_S(x)] = E_S[h_S(x)]$ and $Var_{S|x}[h_S(x)] = Var_S[h_S(x)]$ since $S$ is collected independently from $x$.\n",
    "\n",
    "---\n",
    "\n",
    "**Example.** Let us take the k-nearest neighbors regressor as an example and choose the hypothesis (i.e. estimator) to be $h_S(x) = \\dfrac{1}{k} \\sum_{i=1}^k f(x_i)$, where $\\{(x_1, y_1),\\cdots,(x_k, y_k)\\} \\in S$ are the $k$ nearest neighbors to $x$. Then, we have\n",
    "\n",
    " \\begin{align*}\n",
    "    E_{S,y|x} \\Big [ \\Big(y - h_{S}(x) \\Big)^2 \\Big ]= \\left (E_{y|x} [ y ] - E_{S} \\left [ \\dfrac{1}{k} \\sum_{i=1}^k y_k \\right ] \\right )^2 + Var_{S} \\left [ \\dfrac{1}{k} \\sum_{i=1}^k f(x_k) \\right ]\n",
    "\\end{align*}\n",
    "\n",
    " For a fixed data set $S$, consider the following two cases:\n",
    " \n",
    "   - $k=N$: The estimator is the sample mean. Then $h_S$ will predict the sample mean regardless of $x$, causing high estimator bias and zeros estimator variance. The model is likely to underfit because the sample mean will not explain all the variation in the way $x$ affects $y$.\n",
    "   - $k=1$: The estimator is the nearest neighbor. Then $h_S$ will predict the label of the nearest neighbor regardless of $x$, causing zero estimator bias and high estimator variance. The model is likely to overfit because the hypothesis will incur no error on $S$, while its error on the test set will be dependent on the representativeness of the particular training sample $S$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vapnik - Chervonenkis (VC) Dimension\n",
    "\n",
    "The generalization bound developed above was for a finite hypothesis set. However, in practice we often deal with infinite hypothesis sets. In order to develop generalization bounds for infinite hypothesis sets, we need to define a measure of complexity for these sets. Vapnik and Chervonenkis develops such a measure that from a remarkable observation: In a binary classification problem, a hypothesis set can report a finite number of distinct labelings even though it contains an infinite number of hypotheses. The VC dimension builds on the notions of shattering, which in turn builds on the notion of growth function. Let us describe these notions stepwise and arrive at the definition of VC dimension.\n",
    "\n",
    "**Definition 4 (Growth function)** Given a hypothesis set $\\mathcal{H}$ and a data set $S = \\{(x_1,y_1),...,(x_m,y_m)\\}$, the growth function of $\\mathcal{H}$ with respect to $S$ is defined as\n",
    "\n",
    "$\\Pi_{S,\\mathcal{H}}(m) = \\max_{x_1,...,x_m \\in \\mathcal{X}} |\\{(h(x_1),...,h(x_m)): h \\in \\mathcal{H}\\}|$.\n",
    "\n",
    "In words, the growth function counts the maximum number of distinct labelings that $\\mathcal{H}$ can report on any data set with $m$ data points.\n",
    "\n",
    "**Definition 5 (Shattering)** A hypothesis set $\\mathcal{H}$ shatters a data set $S$ of size $m$ if $\\Pi_{S,\\mathcal{H}}(m) = 2^m$.\n",
    "\n",
    "**Definition 6 (VC dimension)** The VC dimension of a hypothesis set $\\mathcal{H}$ is the size of the largest data set that $\\mathcal{H}$ can shatter:\n",
    "\n",
    "$d_{VC}(\\mathcal{H}) = \\max \\{m: \\Pi_{S,\\mathcal{H}}(m) = 2^m\\}$.\n",
    "\n",
    "Let us conclude the chapter by developing a generalization bound for infinite hypothesis sets. As the first step, we need to bound the growth function of a hypothesis set. The following theorem does exactly that.\n",
    "\n",
    "**Lemma 1 (Sauer's lemma)** Let $\\mathcal{H}$ be a hypothesis set with VC dimension $d_{VC}$. Then for all $m \\geq d_{VC}$, we have\n",
    "\n",
    "$\\Pi_{S,\\mathcal{H}}(m) \\leq \\sum_{i=0}^{d_{VC}} {m \\choose i}$.\n",
    "\n",
    "**Corollary 1** Let $\\mathcal{H}$ be a hypothesis set with VC dimension $d_{VC}$. Then for all $m \\geq d_{VC}$, we have\n",
    "\n",
    "$\\Pi_{S,\\mathcal{H}}(m) \\leq \\left(\\frac{em}{d_{VC}}\\right)^{d_{VC}}$.\n",
    "\n",
    "Using this corollary together with the generalization bound for finite hypothesis sets, we can derive the following generalization bound for infinite hypothesis sets.\n",
    "\n",
    "**Theorem 3 (Generalization bound for infinite hypothesis sets)** Let $\\mathcal{H}$ be a hypothesis set with VC dimension $d_{VC}$. Then for any $\\delta \\in (0,1)$, with probability at least $1-\\delta$ over the choice of $S \\sim D^m$, for all $h \\in \\mathcal{H}$\n",
    "\n",
    "$R(h) \\leq \\widehat{R}_S(h) + \\sqrt{\\dfrac{2 d_{VC} \\log \\frac{em}{d_{VC}} }{m}} + \\sqrt{\\dfrac{\\log \\frac{1}{\\delta}}{2m}}$.\n",
    "\n",
    "The interpretation of this bound is similar to the one for finite hypothesis sets. The bound is uniform, independent of the data distribution and the loss function. It increases logarithmically with the VC dimension and decreases with the size of the training set. The bound is also PAC in the sense that it gives a performance guarantee for any hypothesis in $\\mathcal{H}$.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
